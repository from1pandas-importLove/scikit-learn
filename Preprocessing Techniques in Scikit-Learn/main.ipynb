{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduction",
   "id": "4a709a2e7129481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this lab, we will explore the preprocessing techniques available in scikit-learn. Preprocessing is an essential step in any machine learning workflow as it helps to transform raw data into a suitable format for the learning algorithm. We will cover various preprocessing techniques such as standardization, scaling, normalization, encoding categorical features, imputing missing values, generating polynomial features, and creating custom transformers.",
   "id": "bed92b7cd64ab716"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Standardization",
   "id": "9be08042e8e01377"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Standardization is a common preprocessing step for many machine learning algorithms. It transforms features to have zero mean and unit variance. We can use the StandardScaler from scikit-learn to perform standardization.",
   "id": "5c016f4fb3cc82ea"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-01T20:34:09.829121Z",
     "start_time": "2025-10-01T20:34:08.737255Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1., -1., 2.],\n",
    "              [2., 0., 0.],\n",
    "              [0., 1., -1.]])\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform the training data\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_scaled)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scaling",
   "id": "288317bd4f92ff22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scaling features to a specific range is another common preprocessing technique. It is useful when features have different scales and we want to bring them all to a similar range. The **MinMaxScaler** and **MaxAbsScaler** can be used to perform scaling.",
   "id": "47089951ef4014b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T20:35:12.020149Z",
     "start_time": "2025-10-01T20:35:12.002669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1., -1., 2.],\n",
    "              [2., 0., 0.],\n",
    "              [0., 1., -1.]])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_minmax = min_max_scaler.fit_transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_minmax)\n",
    "\n",
    "# Initialize the MaxAbsScaler\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_maxabs = max_abs_scaler.fit_transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_maxabs)"
   ],
   "id": "c7d0c042b0d629f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         1.        ]\n",
      " [1.         0.5        0.33333333]\n",
      " [0.         1.         0.        ]]\n",
      "[[ 0.5 -1.   1. ]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.  -0.5]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Normalization",
   "id": "ce2ddfd3baca74cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normalization is the process of scaling individual samples to have unit norm. It is commonly used when the magnitude of the data is not important and we are only interested in the direction (or angle) of the data. We can use the **Normalizer** from scikit-learn to perform normalization.",
   "id": "2c24453378c4f67b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T20:36:19.455717Z",
     "start_time": "2025-10-01T20:36:19.432014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[1., -1., 2.],\n",
    "              [2., 0., 0.],\n",
    "              [0., 1., -1.]])\n",
    "\n",
    "# Initialize the Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_normalized)"
   ],
   "id": "5aed0807461f1014",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encoding Categorical Features",
   "id": "9d84e25e836127ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Categorical features need to be encoded into numerical values before they can be used in machine learning algorithms. We can use the **OrdinalEncoder** and **OneHotEncoder** from scikit-learn to encode categorical features.",
   "id": "c12bf35542472fe6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T20:37:09.572197Z",
     "start_time": "2025-10-01T20:37:09.552841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "X = [['male', 'from US', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox']]\n",
    "\n",
    "# Initialize the OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_encoded)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_onehot = onehot_encoder.fit_transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_onehot.toarray())"
   ],
   "id": "4072c7f9aa13812e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 0. 1. 0.]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imputation of Missing Values",
   "id": "aa4b940818672c84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Missing values in a dataset can cause issues with machine learning algorithms. We can use the methods provided in scikit-learn's **impute** module to handle missing values. Here, we will use the **SimpleImputer** to impute missing values.",
   "id": "53921484f9ce254b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T20:38:06.092326Z",
     "start_time": "2025-10-01T20:38:05.717165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "X = np.array([[1., 2., np.nan],\n",
    "              [3., np.nan, 5.],\n",
    "              [np.nan, 4., 6.]])\n",
    "\n",
    "# Initialize the SimpleImputer\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_imputed)"
   ],
   "id": "63d093a74b74e17e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  2.  5.5]\n",
      " [3.  3.  5. ]\n",
      " [2.  4.  6. ]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generating Polynomial Features",
   "id": "7342d9a3d45513a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sometimes it is beneficial to add complexity to a model by considering nonlinear features of the input data. We can use the **PolynomialFeatures** from scikit-learn to generate polynomial features.",
   "id": "1edbd1ecb18b6262"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T20:38:54.639722Z",
     "start_time": "2025-10-01T20:38:54.623684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[0, 1],\n",
    "              [2, 3],\n",
    "              [4, 5]])\n",
    "\n",
    "# Initialize the PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_poly)"
   ],
   "id": "7c4797ca778044a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating Custom Transformers",
   "id": "59355e48cc1a1961"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In some cases, we may want to convert an existing Python function into a transformer to assist in data cleaning or processing. We can achieve this using the **FunctionTransformer** from scikit-learn.",
   "id": "21bf4e8772df6aa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T20:40:11.079047Z",
     "start_time": "2025-10-01T20:40:11.035286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Create a custom function\n",
    "def custom_function(X):\n",
    "    return np.log1p(X)\n",
    "\n",
    "# Initialize the FunctionTransformer\n",
    "transformer = FunctionTransformer(custom_function)\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([[0, 1],\n",
    "              [2, 3]])\n",
    "\n",
    "# Transform the data using the custom function\n",
    "X_transformed = transformer.transform(X)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_transformed)"
   ],
   "id": "c18dace2bf456252",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.69314718]\n",
      " [1.09861229 1.38629436]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summary",
   "id": "7e25d817d7f75fc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Congratulations! You have completed the Preprocessing Data lab. You can practice more labs in LabEx to improve your skills.",
   "id": "aa166b6cdada2c74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb98a8d249d2403"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
