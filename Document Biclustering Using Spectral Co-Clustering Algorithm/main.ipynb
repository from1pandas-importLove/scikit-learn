{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduction",
   "id": "651b65427d905368"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this lab, we will use the Spectral Co-clustering algorithm on the twenty newsgroups dataset to bicluster the documents. The dataset has 20 categories of documents and we will exclude the \"comp.os.ms-windows.misc\" category as it contains posts with no data. The TF-IDF vectorized posts form a word frequency matrix which is then biclustered using Dhillon's Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets of words used more often in those subsets of documents. We will also cluster the documents using MiniBatchKMeans for comparison.",
   "id": "43464b2882a13e1f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-05T21:26:59.483467Z",
     "start_time": "2025-07-05T21:26:59.415578Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import operator\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define Number Normalizer",
   "id": "1748e10ce2e67960"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will define a function **number_normalizer()** to map all numeric tokens to a placeholder. This is used for dimensionality reduction.",
   "id": "18d56bb7ae33ee2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T21:11:44.167834Z",
     "start_time": "2025-07-05T21:11:44.156544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\"Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)"
   ],
   "id": "4506c60672823348",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define NumberNormalizingVectorizer",
   "id": "1aa8bdb1406a3adf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will define a class **NumberNormalizingVectorizer()** that inherits from **TfidfVectorizer()** to build a tokenizer that uses the **number_normalizer()** function we defined earlier.",
   "id": "f9a5fa451ee3eef8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T21:12:58.375771Z",
     "start_time": "2025-07-05T21:12:58.362066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))"
   ],
   "id": "6d3dec1a5d3d661b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and Prepare Data",
   "id": "25608ab39c4d9784"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will load the twenty newsgroups dataset and exclude the \"comp.os.ms-windows.misc\" category. We will also define the vectorizer.",
   "id": "eb0baee5f1443804"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T21:22:18.395961Z",
     "start_time": "2025-07-05T21:13:48.255326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\",\n",
    "    \"comp.windows.x\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"rec.motorcycles\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"rec.sport.hockey\",\n",
    "    \"sci.crypt\",\n",
    "    \"sci.electronics\",\n",
    "    \"sci.med\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.guns\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "y_true = newsgroups.target\n",
    "\n",
    "vectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)"
   ],
   "id": "8092a387fc516b93",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Print Results",
   "id": "d540d61afc94334a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will print the results of the best biclusters found in step 8.",
   "id": "3a26ec11c4e33299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T21:30:02.952599Z",
     "start_time": "2025-07-05T21:29:50.056031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "cocluster = SpectralCoclustering(\n",
    "    n_clusters=len(categories), svd_method=\"arpack\", random_state=0\n",
    ")\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\n",
    ")\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "print(\"Coclustering...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_\n",
    "print(\n",
    "    f\"Done in {time() - start_time:.2f}s. V-measure: \\\n",
    "{v_measure_score(y_cocluster, y_true):.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"MiniBatchKMeans...\")\n",
    "start_time = time()\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "print(\n",
    "    f\"Done in {time() - start_time:.2f}s. V-measure: \\\n",
    "{v_measure_score(y_kmeans, y_true):.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "document_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = Counter(document_names[doc] for doc in cluster_docs)\n",
    "\n",
    "    cat_string = \", \".join(\n",
    "        f\"{(c / n_rows * 100):.0f}% {name}\" for name, c in counter.most_common(3)\n",
    "    )\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = out_of_cluster_docs.nonzero()[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(\n",
    "        word_col[cluster_docs, :].sum(axis=0)\n",
    "        - word_col[out_of_cluster_docs, :].sum(axis=0)\n",
    "    )\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(\n",
    "        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\n",
    "    )\n",
    "\n",
    "    print(f\"bicluster {idx} : {n_rows} documents, {n_cols} words\")\n",
    "    print(f\"categories   : {cat_string}\")\n",
    "    print(f\"words        : {', '.join(important_words)}\\n\")"
   ],
   "id": "c7541294bf0b1d04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Coclustering...\n",
      "Done in 2.13s. V-measure: 0.4415\n",
      "MiniBatchKMeans...\n",
      "Done in 3.66s. V-measure: 0.3015\n",
      "\n",
      "Best biclusters:\n",
      "----------------\n",
      "bicluster 0 : 8 documents, 6 words\n",
      "categories   : 100% talk.politics.mideast\n",
      "words        : cosmo, angmar, alfalfa, alphalpha, proline, benson\n",
      "\n",
      "bicluster 1 : 1948 documents, 4325 words\n",
      "categories   : 23% talk.politics.guns, 18% talk.politics.misc, 17% sci.med\n",
      "words        : gun, guns, geb, banks, gordon, clinton, pitt, cdt, surrender, veal\n",
      "\n",
      "bicluster 2 : 1259 documents, 3534 words\n",
      "categories   : 27% soc.religion.christian, 25% talk.politics.mideast, 25% alt.atheism\n",
      "words        : god, jesus, christians, kent, sin, objective, belief, christ, faith, moral\n",
      "\n",
      "bicluster 3 : 775 documents, 1623 words\n",
      "categories   : 30% comp.windows.x, 25% comp.sys.ibm.pc.hardware, 20% comp.graphics\n",
      "words        : scsi, nada, ide, vga, esdi, isa, kth, s3, vlb, bmug\n",
      "\n",
      "bicluster 4 : 2180 documents, 2802 words\n",
      "categories   : 18% comp.sys.mac.hardware, 16% sci.electronics, 16% comp.sys.ibm.pc.hardware\n",
      "words        : voltage, shipping, circuit, receiver, processing, scope, mpce, analog, kolstad, umass\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summary",
   "id": "3f223d9b2bce51d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this lab, we learned how to perform biclustering using the Spectral Co-clustering algorithm on the twenty newsgroups dataset. We also learned how to cluster the data using MiniBatchKMeans for comparison. Finally, we found the best biclusters by calculating their normalized cut and selecting the top five.",
   "id": "a06c8ca0305bcd1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e757a9a40973c453"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
